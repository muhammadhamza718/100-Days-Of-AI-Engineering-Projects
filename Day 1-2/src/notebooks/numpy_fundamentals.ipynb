{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Cell 6: Random Sampling and Statistical Operations\n# Simulating data and calculating statistics is essential for data analysis and ML.\n\nprint(\\\"=\\\" * 60)\nprint(\\\"SECTION 5: RANDOM SAMPLING & STATISTICS\\\")\\n\",\nprint(\\\"=\\\" * 60)\nprint()\n\nprint(\\\"RANDOM NUMBER GENERATION\\\")\nprint(\\\"-\\\" * 25)\nprint()\n\nprint(\\\"1. Uniform Distribution [0, 1):\")\nuniform_data = np.random.rand(3, 3)\nprint(uniform_data)\nprint()\n\nprint(\\\"2. Standard Normal Distribution (mean=0, std=1):\")\nnormal_data = np.random.randn(3, 3)\nprint(normal_data)\nprint()\n\nprint(\\\"3. Random Integers:\")\nrand_int = np.random.randint(0, 10, size=(3, 3))\nprint(\\\"Random integers between 0 and 10 (inclusive):\\\")\\n\",\nprint(rand_int)\nprint()\n\nprint(\\\"4. Random Choice from Array:\")\nchoices = np.random.choice([10, 20, 30, 40], size=5)\nprint(f\\\"Random choices: {choices}\\\")\nprint()\n\nprint(\\\"STATISTICAL OPERATIONS\\\")\nprint(\\\"-\\\" * 25)\nprint()\n\n# Create a sample dataset\ndata = np.array([[1, 2, 3],\n                 [4, 5, 6],\n                 [7, 8, 9]])\n\nprint(\\\"Sample dataset:\\\")\nprint(data)\nprint()\n\nprint(\\\"Basic Statistics (whole array):\")\nprint(f\\\"Mean: {np.mean(data)}\\\")\\n\",\nprint(f\\\"Standard Deviation: {np.std(data):.4f}\\\")\\n\",\nprint(f\\\"Minimum: {np.min(data)}\\\")\\n\",\nprint(f\\\"Maximum: {np.max(data)}\\\")\\n\",\nprint(f\\\"Sum: {np.sum(data)}\\\")\\n\",\nprint()\n\nprint(\\\"Index-based Operations:\")\nprint(f\\\"Index of maximum value (argmax): {np.argmax(data)}\\\")\\n\",\nprint(f\\\"Index of minimum value (argmin): {np.argmin(data)}\\\")\\n\",\nprint()\n\nprint(\\\"AXIS-SPECIFIC OPERATIONS\\\")\nprint(\\\"-\\\" * 30)\nprint()\nprint(\\\"Understanding axes in 2D arrays:\\\")\nprint(\\\"- axis=0: operate across rows (column-wise)\\\")\\n\",\nprint(\\\"- axis=1: operate across columns (row-wise)\\\")\\n\",\nprint()\n\nprint(f\\\"Original data shape: {data.shape}\\\")\nprint(data)\nprint()\n\nprint(\\\"Sum across columns (axis=0):\")\nsum_axis_0 = np.sum(data, axis=0)\nprint(f\\\"Result: {sum_axis_0}\\\")\\n\",\nprint(f\\\"Shape: {sum_axis_0.shape}\\\")\\n\",\nprint(\\\"Calculation: [1+4+7, 2+5+8, 3+6+9] = [12, 15, 18]\\\")\nprint()\n\nprint(\\\"Sum across rows (axis=1):\")\nsum_axis_1 = np.sum(data, axis=1)\nprint(f\\\"Result: {sum_axis_1}\\\")\\n\",\nprint(f\\\"Shape: {sum_axis_1.shape}\\\")\\n\",\nprint(\\\"Calculation: [1+2+3, 4+5+6, 7+8+9] = [6, 15, 24]\\\")\nprint()\n\nprint(\\\"Mean across columns (axis=0):\")\nmean_axis_0 = np.mean(data, axis=0)\nprint(f\\\"Result: {mean_axis_0}\\\")\\n\",\nprint()\n\nprint(\\\"OTHER USEFUL STATISTICAL FUNCTIONS\\\")\nprint(\\\"-\\\" * 40)\nprint()\n\nprint(\\\"Median:\")\nprint(f\\\"Median (whole array): {np.median(data)}\\\")\\n\",\nprint(f\\\"Median (axis=0): {np.median(data, axis=0)}\\\")\\n\",\nprint()\n\nprint(\\\"Percentiles:\")\nprint(f\\\"25th percentile: {np.percentile(data, 25)}\\\")\\n\",\nprint(f\\\"75th percentile: {np.percentile(data, 75)}\\\")\\n\",\nprint()\n\nprint(\\\"Unique values and counts:\")\nunique_data = np.array([1, 2, 2, 3, 3, 3, 4, 4, 5])\nunique_vals, counts = np.unique(unique_data, return_counts=True)\nprint(f\\\"Unique values: {unique_vals}\\\")\\n\",\nprint(f\\\"Counts: {counts}\\\")\\n\",\nprint()\n\nprint(\\\"Correlation and Covariance:\")\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 4, 6, 8, 10])\nprint(f\\\"Correlation coefficient: {np.corrcoef(x, y)[0, 1]:.4f}\\\")\\n\",\nprint(f\\\"Covariance: {np.cov(x, y)[0, 1]:.4f}\\\")\\n\",\nprint()\n\nprint(\\\"PRACTICAL APPLICATION: DATA NORMALIZATION\\\")\nprint(\\\"-\\\" * 40)\nprint()\n\n# Generate some sample data (e.g., test scores)\nnp.random.seed(42)  # For reproducible results\ntest_scores = np.random.normal(loc=75, scale=10, size=100)\nprint(f\\\"Generated {len(test_scores)} test scores\\\")\\n\",\nprint(f\\\"Mean: {np.mean(test_scores):.2f}\\\")\\n\",\nprint(f\\\"Std Dev: {np.std(test_scores):.2f}\\\")\\n\",\nprint()\n\n# Min-Max normalization to [0, 1]\nmin_val = np.min(test_scores)\nmax_val = np.max(test_scores)\nnormalized_scores = (test_scores - min_val) / (max_val - min_val)\nprint(\\\"Min-Max normalized scores (0 to 1):\\\")\\n\",\nprint(f\\\"Min: {np.min(normalized_scores):.4f}\\\")\\n\",\nprint(f\\\"Max: {np.max(normalized_scores):.4f}\\\")\\n\",\nprint(f\\\"Mean: {np.mean(normalized_scores):.4f}\\\")\\n\",\nprint()\n\n# Z-score normalization (standardization)\nmean_score = np.mean(test_scores)\nstd_score = np.std(test_scores)\nz_scores = (test_scores - mean_score) / std_score\nprint(\\\"Z-scores (mean=0, std=1):\\\")\\n\",\nprint(f\\\"Mean: {np.mean(z_scores):.4f}\\\")\\n\",\nprint(f\\\"Std Dev: {np.std(z_scores):.4f}\\\")\\n\",\nprint()\n\nprint(\\\"KEY INSIGHTS FOR AI/ML:\\\")\nprint(\\\"- Random sampling generates synthetic data for testing\\\")\\n\",\nprint(\\\"- Statistical operations summarize and normalize datasets\\\")\\n\",\nprint(\\\"- Axis operations are crucial for feature engineering\\\")\\n\",\nprint(\\\"- Normalization ensures features are on similar scales\\\")\\n\",\nprint(\\\"- Understanding distributions helps detect anomalies\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 5: Linear Algebra Operations\n# Linear algebra is at the heart of Deep Learning and machine learning algorithms.\n\nprint(\\\"=\\\" * 60)\nprint(\\\"SECTION 4: LINEAR ALGEBRA OPERATIONS\\\")\\n\",\nprint(\\\"=\\\" * 60)\nprint()\n\nprint(\\\"1. DOT PRODUCT\\\")\nprint(\\\"-\\\" * 20)\nprint(\\\"The dot product is the sum of the products of corresponding entries.\\\")\nprint()\n\na = np.array([1, 2])\nb = np.array([3, 4])\n\nprint(f\\\"a = {a}\\\")\nprint(f\\\"b = {b}\\\")\nprint()\n\n# Method 1: Using np.dot()\ndot_product = np.dot(a, b)\nprint(f\\\"Dot product (np.dot): {dot_product}\\\")\nprint(f\\\"Calculation: 1*3 + 2*4 = {dot_product}\\\")\nprint()\n\n# Method 2: Using the @ operator (Python 3.5+)\ndot_product_v2 = a @ b\nprint(f\\\"Dot product (@ operator): {dot_product_v2}\\\")\nprint()\n\nprint(\\\"2. MATRIX MULTIPLICATION\\\")\nprint(\\\"-\\\" * 20)\nprint()\n\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\nprint(\\\"Matrix A:\\\")\nprint(A)\nprint()\nprint(\\\"Matrix B:\\\")\nprint(B)\nprint()\n\n# Method 1: Using np.matmul()\nC = np.matmul(A, B)\nprint(\\\"Matrix multiplication (np.matmul):\")\nprint(C)\nprint()\n\n# Method 2: Using @ operator\nD = A @ B\nprint(\\\"Matrix multiplication (@ operator):\")\nprint(D)\nprint()\n\nprint(\\\"Mathematical verification:\\\")\nprint(\\\"C[0,0] = 1*5 + 2*7 =\\\", C[0, 0], \\\"(expected: 19)\\\")\nprint(\\\"C[0,1] = 1*6 + 2*8 =\\\", C[0, 1], \\\"(expected: 22)\\\")\nprint(\\\"C[1,0] = 3*5 + 4*7 =\\\", C[1, 0], \\\"(expected: 43)\\\")\nprint(\\\"C[1,1] = 3*6 + 4*8 =\\\", C[1, 1], \\\"(expected: 50)\\\")\nprint()\n\nprint(\\\"3. OTHER COMMON LINEAR ALGEBRA OPERATIONS\\\")\nprint(\\\"-\\\" * 40)\nprint()\n\nprint(\\\"Matrix Inverse (A^-1):\")\nA_inv = np.linalg.inv(A)\nprint(A_inv)\nprint()\n\nprint(\\\"Eigenvalues and Eigenvectors:\")\neigenvalues, eigenvectors = np.linalg.eig(A)\nprint(f\\\"Eigenvalues: {eigenvalues}\\\")\nprint(f\\\"Eigenvectors:\\\\n{eigenvectors}\\\")\nprint()\n\nprint(\\\"Matrix Norm (magnitude):\")\nnorm = np.linalg.norm(A)\nprint(f\\\"Frobenius norm of A: {norm:.4f}\\\")\nprint()\n\nprint(\\\"Transpose:\")\nprint(\\\"Original A:\\\")\nprint(A)\nprint()\nprint(\\\"Transposed A (A.T):\")\nprint(A.T)\nprint()\n\nprint(\\\"4. MATRIX PROPERTIES\\\")\nprint(\\\"-\\\" * 20)\nprint()\n\nprint(f\\\"Matrix A shape: {A.shape}\\\")\nprint(f\\\"Matrix A size (total elements): {A.size}\\\")\nprint(f\\\"Matrix A data type: {A.dtype}\\\")\nprint()\n\nprint(\\\"Identity Matrix:\\\")\nidentity_3x3 = np.eye(3)\nprint(identity_3x3)\nprint()\n\nprint(\\\"KEY CONCEPTS FOR AI/ML:\\\")\nprint(\\\"- Dot products measure similarity between vectors\\\")\\n\",\nprint(\\\"- Matrix multiplication is fundamental to neural networks\\\")\\n\",\nprint(\\\"- Eigenvalues/eigenvectors used in dimensionality reduction (PCA)\\\")\\n\",\nprint(\\\"- Matrix inverses important for solving systems of equations\\\")\\n\",\nprint(\\\"- All operations are highly optimized in NumPy for performance\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 4: Broadcasting Explanation and Examples\n# WHAT IS BROADCASTING? Broadcasting allows NumPy to work with arrays of different shapes\n# during arithmetic operations. The smaller array is \"broadcast\" across the larger array.\n\nprint(\\\"=\\\" * 60)\nprint(\\\"SECTION 3: BROADCASTING - OPERATIONS ON DIFFERENT SHAPES\\\")\\n\",\nprint(\\\"=\\\" * 60)\nprint()\n\nprint(\\\"BROADCASTING RULES:\\\")\nprint(\\\"1. If arrays don't have the same rank, prepend shape of lower rank with 1s\\\")\nprint(\\\"2. Arrays are compatible in a dimension if they have the same size, OR\\\")\nprint(\\\"   if one of the arrays has size 1 in that dimension\\\")\nprint()\n\nprint(\\\"EXAMPLE 1: Broadcasting 1D array to 2D array\\\")\nprint(\\\"-\\\" * 40)\n\n# Create arrays with different shapes\nA = np.array([[1, 2, 3],\n              [4, 5, 6]])  # Shape: (2, 3)\nb = np.array([10, 20, 30]) # Shape: (3,)\n\nprint(f\\\"Array A (shape {A.shape}):\\\")\\n\",\nprint(A)\nprint()\nprint(f\\\"Array b (shape {b.shape}): {b}\\\")\nprint()\n\n# Broadcasting b to match A's shape\nprint(\\\"A + b (broadcasting b across rows):\\\")\\n\",\nresult = A + b\nprint(result)\nprint(f\\\"Result shape: {result.shape}\\\")\nprint()\n\nprint(\\\"WHAT HAPPENED UNDER THE HOOD?\\\")\nprint(\\\"NumPy 'stretched' b from shape (3,) to shape (1, 3) then to (2, 3):\\\")\\n\",\nprint(\\\"b (original): [10, 20, 30]\\\")\nprint(\\\"b (broadcast to match A):\\\")\\n\",\nprint(\\\"[[10, 20, 30],  <- original b\\\")\nprint(\\\" [10, 20, 30]]  <- broadcast copy of b\\\")\nprint()\n\nprint(\\\"EXAMPLE 2: Broadcasting with different dimensions\\\")\nprint(\\\"-\\\" * 40)\n\n# Create a column vector (2D array with shape (3, 1))\ncol_vector = np.array([[1], [2], [3]])  # Shape: (3, 1)\nrow_vector = np.array([10, 20, 30])     # Shape: (3,)\n\nprint(f\\\"Column vector (shape {col_vector.shape}):\\\")\\n\",\nprint(col_vector)\nprint()\nprint(f\\\"Row vector (shape {row_vector.shape}): {row_vector}\\\")\nprint()\n\ntry:\n    result = col_vector + row_vector\n    print(\\\"col_vector + row_vector:\\\")\n    print(result)\n    print(f\\\"Result shape: {result.shape}\\\")\n    print()\n    print(\\\"WHY THIS WORKS:\\\")\n    print(\\\"- col_vector has shape (3, 1)\\\")\n    print(\\\"- row_vector has shape (3,) -> becomes (1, 3) for broadcasting\\\")\n    print(\\\"- Compatible dimensions: 3==3 (rows) and 1==3 (columns via broadcasting)\\\")\n    print(\\\"- Final result shape: (3, 3)\\\")\nexcept Exception as e:\n    print(f\\\"Error: {e}\\\")\n    print(\\\"This combination might not be compatible for broadcasting.\\\")\nprint()\n\nprint(\\\"EXAMPLE 3: Practical use case - Normalizing a matrix\\\")\nprint(\\\"-\\\" * 40)\n\n# Create a data matrix (e.g., 3 features, 5 samples)\ndata = np.random.rand(5, 3) * 100  # Shape: (5, 3)\nprint(f\\\"Original data (5 samples Ã— 3 features), shape {data.shape}:\\\")\\n\",\nprint(data.round(2))\nprint()\n\n# Calculate column means (axis=0: operate across rows for each column)\nmeans = np.mean(data, axis=0)\nprint(f\\\"Column means (shape {means.shape}): {means.round(2)}\\\")\nprint()\n\n# Subtract means from each column using broadcasting\nnormalized = data - means\nprint(\\\"Data minus column means (centered data):\\\")\\n\",\nprint(normalized.round(2))\nprint(f\\\"Shape: {normalized.shape}\\\")\nprint()\n\nprint(\\\"KEY TAKEAWAYS:\\\")\nprint(\\\"- Broadcasting eliminates need for explicit loops or replication\\\")\\n\",\nprint(\\\"- Makes code cleaner and much faster\\\")\\n\",\nprint(\\\"- Fundamental to efficient NumPy programming\\\")\\n\",\nprint(\\\"- Understanding rules prevents errors and enables elegant solutions\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 3: Vectorization Performance Comparison\n# WHY THIS MATTERS: Vectorization enables batch operations on data without writing any for loops.\n# This is not only more concise but significantly faster due to low-level C optimizations.\n\nprint(\\\"=\\\" * 60)\nprint(\\\"SECTION 2: VECTORIZATION - THE POWER OF NUMPY\\\")\\n\",\nprint(\\\"=\\\" * 60)\nprint()\n\nprint(\\\"Let's compare two approaches for calculating the sum of squares:\\\")\nprint()\n\n# Example data\ntest_array = np.random.rand(10000)\nprint(f\\\"Test array size: {len(test_array)} elements\\\")\nprint()\n\n# Approach 1: Traditional Python loop (SLOW - educational purposes only)\ndef sum_squares_loop(arr):\n    \\\"\\\"\\\"\n    Non-vectorized implementation using Python loop.\n    This demonstrates why we avoid loops for numerical operations.\n    \\\"\\\"\\\"\n    result = 0\n    for x in arr:\n        result += x ** 2\n    return result\n\n# Approach 2: NumPy vectorized operation (FAST - recommended approach)\ndef sum_squares_numpy(arr):\n    \\\"\\\"\\\"\n    Vectorized implementation using NumPy operations.\n    WHY: np.sum operates at C speed, optimized for array operations.\n    The expression arr ** 2 creates the entire result array efficiently.\n    \\\"\\\"\\\"\n    return np.sum(arr ** 2)\n\nprint(\\\"Testing performance...\\\")\nprint()\n\n# Performance comparison using timeit\n# We run each function 100 times to get reliable timing measurements\nloop_time = timeit(lambda: sum_squares_loop(test_array), number=100)\nnumpy_time = timeit(lambda: sum_squares_numpy(test_array), number=100)\n\nprint(f\\\"Loop implementation:    {loop_time:.4f} seconds (100 runs)\\\")\nprint(f\\\"NumPy implementation:   {numpy_time:.4f} seconds (100 runs)\\\")\nprint()\n\n# Calculate performance improvement\nimprovement = loop_time / numpy_time\nprint(f\\\"Performance improvement: {improvement:.1f}x FASTER with NumPy\\\")\nprint()\n\nprint(\\\"KEY INSIGHTS:\\\")\nprint(\\\"- Vectorized operations eliminate Python loop overhead\\\")\nprint(\\\"- NumPy uses highly optimized C code under the hood\\\")\nprint(\\\"- This 10-100x speedup is typical for numerical computations\\\")\nprint(\\\"- ALWAYS prefer vectorized operations in production code\\\")\nprint()\n\nprint(\\\"Verification - both approaches give the same result:\\\")\nresult_loop = sum_squares_loop(test_array[:100])  # Small subset for quick check\nresult_numpy = sum_squares_numpy(test_array[:100])\nprint(f\\\"Loop result:    {result_loop:.6f}\\\")\nprint(f\\\"NumPy result:   {result_numpy:.6f}\\\")\nprint(f\\\"Results match:  {np.isclose(result_loop, result_numpy)}\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 2: Array Creation and Basic Operations\n\n# NumPy's main object is the homogeneous multidimensional array.\n# It's a table of elements (usually numbers), all of the same type,\n# indexed by a tuple of non-negative integers.\n\nprint(\\\"Creating different dimensional arrays:\\\")\n\n# 1D array (vector)\narray_1d = np.array([1, 2, 3, 4, 5])\nprint(f\\\"1D Array: {array_1d}\\\")\nprint(f\\\"Shape: {array_1d.shape}\\\")\nprint(f\\\"Dimensions: {array_1d.ndim}\\\")\nprint()\n\n# 2D array (matrix)\narray_2d = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\\\"2D Array:\\\")\nprint(array_2d)\nprint(f\\\"Shape: {array_2d.shape}\\\")\nprint(f\\\"Dimensions: {array_2d.ndim}\\\")\nprint()\n\n# 3D array (tensor)\narray_3d = np.random.rand(2, 3, 3)\nprint(\\\"3D Array (random values):\")\nprint(array_3d)\nprint(f\\\"Shape: {array_3d.shape}\\\")\nprint(f\\\"Dimensions: {array_3d.ndim}\\\")\nprint()\n\nprint(\\\"-\\\" * 50)\nprint(\\\"Basic Arithmetic Operations (Element-wise):\\\")\n\n# Basic arithmetic operations are performed element-wise\nx = np.array([10, 20, 30])\ny = np.array([1, 2, 3])\n\nprint(f\\\"x = {x}\\\")\nprint(f\\\"y = {y}\\\")\nprint()\n\nprint(f\\\"Addition: x + y = {x + y}\\\")        # [11 22 33]\nprint(f\\\"Subtraction: x - y = {x - y}\\\")     # [ 9 18 27]\nprint(f\\\"Multiplication: x * y = {x * y}\\\")  # [10 40 90]\nprint(f\\\"Division: x / y = {x / y}\\\")        # [10. 10. 10.]\nprint()\n\nprint(\\\"Array shapes and data types:\\\")\nprint(f\\\"x shape: {x.shape}, dtype: {x.dtype}\\\")\nprint(f\\\"y shape: {y.shape}, dtype: {y.dtype}\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy Mastery - Day 1-2 Learning Module\n",
    "\n",
    "**Welcome to your NumPy learning journey!** This notebook will guide you through:\n",
    "\n",
    "1. **Core Array Operations** - Creating and manipulating NumPy arrays\n",
    "2. **Vectorization** - Understanding the performance benefits of NumPy\n",
    "3. **Broadcasting** - Working with arrays of different shapes\n",
    "4. **Linear Algebra** - Dot products, matrix multiplication, and more\n",
    "5. **Statistics** - Random sampling and statistical operations\n",
    "\n",
    "**Learning Goals:**\n",
    "- Master array creation, shapes, and types\n",
    "- Deep understanding of broadcasting rules\n",
    "- Competence in basic linear algebra using NumPy\n",
    "- Ability to perform random sampling and calculate basic statistics\n",
    "\n",
    "**Remember:** NumPy's main object is the homogeneous multidimensional array - a table of elements (usually numbers), all of the same type, indexed by a tuple of non-negative integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "# This cell must be run first to import all required libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import timeit\n",
    "import sys\n",
    "\n",
    "print(\"NumPy Mastery - Day 1-2 Learning Module\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print()\n",
    "print(\"All imports successful! Ready to begin.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}